<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://leuraph.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://leuraph.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-13T15:59:10+00:00</updated><id>https://leuraph.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A new Interpretation of the Jacobi method</title><link href="https://leuraph.github.io/blog/2024/jacobi_and_line_search/" rel="alternate" type="text/html" title="A new Interpretation of the Jacobi method"/><published>2024-02-13T15:00:00+00:00</published><updated>2024-02-13T15:00:00+00:00</updated><id>https://leuraph.github.io/blog/2024/jacobi_and_line_search</id><content type="html" xml:base="https://leuraph.github.io/blog/2024/jacobi_and_line_search/"><![CDATA[<h1 id="putting-things-into-context">Putting things into context</h1> <p>If you are familiar with the terminology of <em>line search</em> and know what the Jacobi method is, please feel free to skip directly to <a href="#a-new-way-of-interpreting-the-jacobi-method">a new way of interpreting the Jacobi method</a>.</p> <hr/> <p>Consider a set of linear equations, i.e.</p> <p>\begin{equation} \label{eq:problem} Ax = b, \end{equation}</p> <p>where \(A\) is a positive-definite, symmetric, and real \(N \times N\) matrix. It can be shown that finding a solution to eq. \eqref{eq:problem} is equivalent to finding the minimizer of the target function</p> <p>\begin{equation} \label{eq:minimization} F(x) := \frac{1}{2} x^T A x - b^T x. \end{equation}</p> <p><em>Iterative methods</em> try to find an approximation of the solution of eq. \eqref{eq:problem} by finding a sequence \(\{ x_k \}_k \subset \mathbb{R}^N\) that eventually minimizes the target function \(F\), i.e. \(F(x_{k}) \to F(x^*) = \min_{x\in\mathbb{R}^N} F(x)\), Generally, one may think of obtaining a sequence of iterates \(\{ x_k \}_k \subset \mathbb{R}^N\) in the following way. Given an initial value \(x_0 \in \mathbb{R}^N\), we iteratively define</p> <p>\begin{equation} \label{eq:algo} x_{k+1} := x_k + d_k, \end{equation}</p> <p>where \(d_k := x_{i+1} - x_i \in \mathbb{R}^N\). How one chooses \(d_i\) is what distinguishes different iterative algorithms from each other. In the following, we outline possible choices of the direction and length of \(d_k\) and finally draw the connection to an alternative interpretation of the Jacobi method.</p> <h2 id="the-method-of-gradient-descent">The method of Gradient Descent</h2> <p>The method of Gradient Descent is an iterative method operating along the lines of eq. \eqref{eq:algo}. In every iteration, the method of Gradient descent moves along the negative gradient of the target function \(F\), i.e. given an initial guess \(x_0 \in \mathbb{R}^N\), one does</p> \[x_{k+1} = x_k - \alpha_k \nabla F(x_k)\] <p>where \(-\nabla F(x_k) = b - A x_k =: r_k\) is the so-called residual. There are several ways of choosing the step size \(\alpha_k\). One of which is found by performing a <em>line search</em> along the direction of \(r_k\).</p> <h2 id="line-search">Line Search</h2> <p>Given an iterate \(x_k\) and a direction \(d_k\), a line search minimizes the target function \(F\) along the line \(\tau \mapsto x_k + \tau d_k,\,\tau\in\mathbb{R}\), i.e. we choose the step size \(\tau\) by requiring \(\frac{\mathrm{d}}{\mathrm{d}\tau} F(x_k + \tau d_k) = 0,\) yielding,</p> \[\tau_k := \frac{d_k^T r_k}{d_k^T A d_k}.\] <hr/> <h1 id="the-jacobi-method">The Jacobi Method</h1> <p>The Jacobi method is another iterative method operating along the lines of eq. \eqref{eq:algo}. Namely, the Jacobi method is given by</p> <p>\begin{equation} \label{eq:jacobi} x_{k+1} = x_k + \mathrm{diag}(A)^{-1} r_k. \end{equation}</p> <p>We now present the classic motivation and, finally, a new way of interpreting the Jacobi method with the guidance of line search.</p> <h2 id="the-classical-motivation-of-the-jacobi-method">The classical motivation of the Jacobi method</h2> <p>Note that any matrix \(A\) can be written as \(A = \mathrm{diag}(A) + (A - \mathrm{diag}(A))\). Defining \(D:=\mathrm{diag}(A)\), we then rewrite eq. \eqref{eq:problem} as</p> \[Ax = b \Leftrightarrow x = D^{-1} \Big[ \underbrace{b - Ax}_{=r} + Dx \Big] = x + D^{-1}r.\] <p>This motivates the iterative method given by</p> \[x_{k+1} = x_k + D^{-1}r_k,\] <p>also known as the Jacobi method.</p> <h2 id="a-new-way-of-interpreting-the-jacobi-method">A new way of interpreting the Jacobi method</h2> <p>Consider an iterate \(x_k\). Now, instead of performing an iteration step along the lines of eq. \eqref{eq:algo}, we calculate the ideal step size for each canonical direction, i.e. we do a line search for each canonical basis vector \(e_i\). Namely, for each \(i \in \{ 1, \dots, N \}\), we calculate</p> \[\alpha_i = \frac{e_i^T r_k}{e_i^T A e_i} = \frac{r_k^i}{A_ii},\] <p>where \(r_k^i\) denotes the \(i\)-th entry of the vector \(r_k\). Then, we collect all of these values in a single direction vector \(d_k\), recovering the Jacobi method in doing so. Namely, we define</p> \[d_k := \sum_i \alpha_i e_i=\mathrm{diag}(A)^{-1} r_k.\]]]></content><author><name></name></author><category term="math"/><category term="numerical-analysis"/><summary type="html"><![CDATA[Interpreting the Jacobi method as a collection of line searches]]></summary></entry></feed>